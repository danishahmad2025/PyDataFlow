# PyDataFlow ğŸš€

A beginner-friendly, production-style **Python data ingestion pipeline**.

---

## ğŸ“Œ Project Overview

**PyDataFlow** is a mini data engineering project that demonstrates how
real-world data pipelines are built using **pure Python**.

This project is designed for:
- Learning core data engineering concepts
- Practicing clean project structure
- Understanding streaming-style pipelines
- Revision and open-source contribution

---

## ğŸ¯ What This Project Does

- Reads data from **CSV** and **JSON** files
- Processes records **one at a time** using generators (memory efficient)
- Validates each record safely without stopping the pipeline
- Separates **valid** and **rejected** records
- Stores rejected records with detailed rejection reasons
- Tracks basic processing metrics
- Logs every important step for debugging and observability

---

## ğŸ§  Why This Project Matters

In real-world data systems:

- Data is often **messy or incomplete**
- Pipelines **must not crash** because of bad records
- Logging and metrics are essential for debugging
- Code must be **modular, readable, and scalable**

**PyDataFlow simulates these real-world challenges** in a simple and easy-to-understand way.

---

## ğŸ—ï¸ Project Structure

PyDataFlow/
â”‚
â”œâ”€â”€ ingestion/ # Reading data sources
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ reader.py # CSV & JSON readers (generators)
â”‚
â”œâ”€â”€ validation/ # Data validation logic
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ validator.py
â”‚
â”œâ”€â”€ storage/ # Handling rejected records
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ rejected_writer.py
â”‚
â”œâ”€â”€ logging_config/ # Centralized logging setup
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Input data files
â”‚ â””â”€â”€ rejected/ # Rejected records (not committed)
â”‚
â”œâ”€â”€ utils/ # Utilities (retry, metrics, helpers)
â”‚
â”œâ”€â”€ main.py # Pipeline entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ”§ Technologies Used

- **Python**
- Python **logging** module
- Generators & iterators
- File-based processing (CSV, JSON, JSONL)
- Modular pipeline architecture
- Git & GitHub

> âš ï¸ **Note:**  
> This project intentionally avoids Pandas to demonstrate
> streaming data processing and memory-efficient design.

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/PyDataFlow.git
cd PyDataFlow

---

2ï¸âƒ£ (Optional) Create a virtual environment
python -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the pipeline
python main.py

ğŸ“Š Output You Will See

Logs printed in the terminal

Logs stored in log files

Valid records processed successfully

Invalid records written to:

data/rejected/rejected_records.jsonl


Final execution summary with processing metrics

ğŸ§ª Example Rejected Record (JSONL)
{
  "timestamp": "2025-12-30T19:18:49",
  "source": "csv",
  "reason": "Invalid age value",
  "record": {
    "id": "5",
    "name": "David",
    "age": "-5"
  }
}

ğŸŒ± Future Improvements

Add database storage (SQLite / PostgreSQL)

Add Pandas-based transformations

Add unit tests

Add orchestration (Airflow / Prefect)

Add API-based ingestion

ğŸ¤ Contributions

This project is open-source and contribution-friendly.

You are welcome to:

Fork the repository

Open issues

Submit pull requests

Suggest improvements

ğŸ“Œ Author

Danish Ekbal Ahmad
Learning Data Engineering by building real-world projects.

